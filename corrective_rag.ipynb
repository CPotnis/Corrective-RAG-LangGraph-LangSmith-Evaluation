{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Library Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langsmith import Client\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Set the current working directory and file path\n",
    "current_dir = os.getcwd()  \n",
    "file_path = os.path.join(current_dir, \"data\", \"aws-overview.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Model Initialization and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    web_search_tool = TavilySearchResults()\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an assistant for question-answering tasks. \n",
    "        Use the following documents to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Use three sentences maximum and keep the answer concise:\n",
    "        Question: {question} \n",
    "        Documents: {documents} \n",
    "        Answer: \n",
    "        \"\"\",\n",
    "        input_variables=[\"question\", \"documents\"],\n",
    "    )\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    llm_json = ChatOllama(model=\"llama3.1\", format=\"json\", temperature=0)\n",
    "\n",
    "    grader_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant.\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grader_prompt | llm_json | JsonOutputParser()\n",
    "    \n",
    "    return web_search_tool, rag_chain, retrieval_grader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Corrective RAG Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    search: str\n",
    "    documents: List[str]\n",
    "    steps: List[str]\n",
    "\n",
    "def retrieve(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"retrieve_documents\")\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "\n",
    "def grade_documents(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"grade_document_retrieval\")\n",
    "    filtered_docs = []\n",
    "    search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"search\": search, \"steps\": steps}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    search = state[\"search\"]\n",
    "    return \"search\" if search == \"Yes\" else \"generate\"\n",
    "\n",
    "def web_search(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"web_search\")\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "    documents.extend([Document(page_content=d[\"content\"], metadata={\"url\": d[\"url\"]}) for d in web_results])\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "\n",
    "def generate(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"generate_answer\")\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"steps\": steps}\n",
    "\n",
    "def build_graph():\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"retrieve\", retrieve)\n",
    "    workflow.add_node(\"grade_documents\", grade_documents)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "    workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        decide_to_generate,\n",
    "        {\"search\": \"web_search\", \"generate\": \"generate\"},\n",
    "    )\n",
    "    workflow.add_edge(\"web_search\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Example Usage of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_custom_agent_answer(example: dict):\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    state_dict = custom_graph.invoke({\"question\": example[\"input\"], \"steps\": []}, config)\n",
    "    return {\"response\": state_dict[\"generation\"], \"steps\": state_dict[\"steps\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: LangSmith Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_langsmith_evaluation():\n",
    "    client = Client()\n",
    "\n",
    "    examples = [\n",
    "        (\"What is AWS EC2?\", \"AWS EC2 (Elastic Compute Cloud) is a web service that provides resizable compute capacity in the cloud. It allows users to rent virtual servers (instances) to run applications, offering scalability and flexibility. EC2 is a core component of Amazon's cloud computing platform, enabling businesses to quickly deploy and manage computing resources without investing in physical hardware.\"),\n",
    "        (\"What AWS services can I use to host a Python web app?\", \"For hosting a Python web app on AWS, you can use AWS Elastic Beanstalk for a fully managed solution, EC2 for more control over your environment, or Lambda with API Gateway for a serverless approach. The choice depends on your app's requirements, desired level of control, and scalability needs.\")\n",
    "    ]\n",
    "\n",
    "    dataset_name = \"Corrective RAG Agent Testing\"\n",
    "    if not client.has_dataset(dataset_name=dataset_name):\n",
    "        dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "        inputs, outputs = zip(*[({\n",
    "            \"input\": text\n",
    "        }, {\n",
    "            \"output\": label\n",
    "        }) for text, label in examples])\n",
    "        client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
    "    \n",
    "    return dataset_name\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    score = answer_grader.invoke({\n",
    "        \"question\": input_question,\n",
    "        \"correct_answer\": reference,\n",
    "        \"student_answer\": prediction,\n",
    "    })\n",
    "    score = score[\"Score\"]\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n",
    "\n",
    "def check_trajectory_custom(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in exact order and without any additional tool calls.\n",
    "    \"\"\"\n",
    "    expected_trajectory_1 = [\n",
    "        \"retrieve_documents\",\n",
    "        \"grade_document_retrieval\",\n",
    "        \"web_search\",\n",
    "        \"generate_answer\",\n",
    "    ]\n",
    "    expected_trajectory_2 = [\n",
    "        \"retrieve_documents\",\n",
    "        \"grade_document_retrieval\",\n",
    "        \"generate_answer\",\n",
    "    ]\n",
    "    \n",
    "    tool_calls = root_run.outputs[\"steps\"]\n",
    "    print(f\"Tool calls custom agent: {tool_calls}\")\n",
    "    if tool_calls == expected_trajectory_1 or tool_calls == expected_trajectory_2:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return {\"score\": int(score), \"key\": \"tool_calls_in_exact_order\"}\n",
    "\n",
    "def run_evaluation(dataset_name):\n",
    "    model_tested = \"llama3.1\"\n",
    "    metadata = \"CRAG, llama3.1\"\n",
    "    experiment_prefix = f\"custom-agent-{model_tested}\"\n",
    "    \n",
    "    experiment_results = evaluate(\n",
    "        predict_custom_agent_answer,\n",
    "        data=dataset_name,\n",
    "        evaluators=[answer_evaluator, check_trajectory_custom],\n",
    "        experiment_prefix=experiment_prefix + \"-answer-and-tool-use\",\n",
    "        num_repetitions=3,\n",
    "        max_concurrency=1,\n",
    "        metadata={\"version\": metadata},\n",
    "    )\n",
    "    \n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the system...\n",
      "Building the graph...\n",
      "\n",
      "Running example queries:\n",
      "\n",
      "Example 1:\n",
      "Question: What AWS services can I use to host a Python web app?\n",
      "Answer: You can use the following AWS services to host a Python web app:\n",
      "\n",
      "* Amazon Elastic Compute Cloud (EC2)\n",
      "* AWS Elastic Beanstalk\n",
      "* AWS App Runner\n",
      "* CodePipeline and GitHub for CI/CD pipeline automation.\n",
      "\n",
      "These services provide scalable, secure, and managed infrastructure for deploying and running your Python application.\n",
      "Steps: ['retrieve_documents', 'grade_document_retrieval', 'web_search', 'generate_answer']\n",
      "\n",
      "Example 2:\n",
      "Question: What is AWS EC2?\n",
      "Answer: Amazon EC2 (Elastic Compute Cloud) is a web service that provides secure, resizable compute capacity in the cloud. It allows developers to obtain and configure capacity with minimal friction, providing complete control over computing resources. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing for quick scaling of capacity as needed.\n",
      "Steps: ['retrieve_documents', 'grade_document_retrieval', 'generate_answer']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Initialize the system\n",
    "    print(\"Initializing the system...\")\n",
    "    retriever = load_and_process_data(file_path)\n",
    "    web_search_tool, rag_chain, retrieval_grader = initialize_models()\n",
    "\n",
    "    # Step 2: Build the graph\n",
    "    print(\"Building the graph...\")\n",
    "    custom_graph = build_graph()\n",
    "\n",
    "    # Step 3: Run example queries\n",
    "    print(\"\\nRunning example queries:\")\n",
    "    examples = [\n",
    "        {\"input\": \"What AWS services can I use to host a Python web app?\"},\n",
    "        {\"input\": \"What is AWS EC2?\"}\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        response = predict_custom_agent_answer(example)\n",
    "        print(f\"Question: {example['input']}\")\n",
    "        print(f\"Answer: {response['response']}\")\n",
    "        print(f\"Steps: {response['steps']}\")\n",
    "\n",
    "    # Step 4: Set up and run LangSmith evaluation\n",
    "    # print(\"\\nSetting up LangSmith evaluation...\")\n",
    "    # dataset_name = setup_langsmith_evaluation()\n",
    "    \n",
    "    # print(\"\\nRunning evaluation...\")\n",
    "    # evaluation_results = run_evaluation(dataset_name)\n",
    "    \n",
    "    # print(\"\\nEvaluation complete. Results:\")\n",
    "    # print(evaluation_results)\n",
    "\n",
    "    # print(\"\\nYou can view detailed results in the LangSmith dashboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
